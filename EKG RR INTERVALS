import numpy as np
import tensorflow as tf
#import matplotlib.pyplot as plt
from tensorflow.python.framework import ops
from tensorflow.python.ops import clip_ops

num_filt_1 = 16     #Number of filters in first conv layer #32
num_filt_2 = 14      #Number of filters in second conv layer #28
num_filt_3 = 8      #Number of filters in thirs conv layer #16
num_fc_1 = 40       #Number of neurons in hully connected layer #80
max_iterations = 20000
BATCH_SIZE = 64 #64
dropout = 1.0       #Dropout rate in the fully connected layer
#plot_row = 5        #How many rows do you want to plot in the visualization
learning_rate = 2e-5
input_norm = False   # Do you want z-score input normalization?
num_classes=2




IRIS_TEST = "TESTRR20V4.csv"




test_set=tf.contrib.learn.datasets.base.load_csv_with_header(
	filename=IRIS_TEST,
	target_dtype=np.int,
	features_dtype=np.float32,
	target_column=-1)




N_FEATURES=20


def batch_generator(filenames):
	filename_queue=tf.train.string_input_producer(filenames)
	reader=tf.TextLineReader(skip_header_lines=0)
	_,value=reader.read(filename_queue)
	record_defaults=[[1.0] for _ in range(N_FEATURES)]
	record_defaults.append([1])
	content=tf.decode_csv(value, record_defaults=record_defaults)
	features=tf.stack(content[:N_FEATURES])
	label=content[-1]
	min_after_dequeue=10*BATCH_SIZE #10
	capacity=20*BATCH_SIZE          #20
	data_batch, label_batch = tf.train.shuffle_batch([features, label], batch_size=BATCH_SIZE, capacity=capacity, min_after_dequeue=min_after_dequeue)
	return data_batch, label_batch

def batchnorm(Ylogits, is_test, iteration, offset, convolutional=False): 
    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations 
    bnepsilon = 1e-5 
    if convolutional: 
        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2]) 
    else: 
        mean, variance = tf.nn.moments(Ylogits, [0]) 
    update_moving_everages = exp_moving_avg.apply([mean, variance]) 
    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean) 
    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance) 
    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon) 
    return Ybn, update_moving_everages 

def compatible_convolutional_noise_shape(Y): 
    noiseshape = tf.shape(Y) 
    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0]) 
    return noiseshape




#epochs = np.floor(batch_size*max_iterations / N)
#print('Train with approximately %d epochs' %(epochs))

tf.set_random_seed(0.0) 

X = tf.placeholder(tf.float32, shape=[None, N_FEATURES], name = 'Input_data')
y_ = tf.placeholder(tf.float32, shape=[None,2], name = 'Ground_truth')
pkeep = tf.placeholder("float")
tst = tf.placeholder(tf.bool)

iter = tf.placeholder(tf.int32) 


def bias_variable(shape, name):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial, name = name)


def conv2d(X, W):
  return tf.nn.conv2d(X, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(X):
  return tf.nn.max_pool(X, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding='SAME')


with tf.name_scope("Reshaping_data") as scope:
  x_image = tf.reshape(X, [-1,N_FEATURES,1,1])


initializer = tf.contrib.layers.xavier_initializer()


with tf.name_scope("Conv1") as scope:
  W_conv1 = tf.get_variable("Conv_Layer_1", shape=[5, 1, 1, num_filt_1],initializer=initializer) #10
  b_conv1 = bias_variable([num_filt_1], 'bias_for_Conv_Layer_1')
  a_conv1 = conv2d(x_image, W_conv1) + b_conv1


with tf.name_scope('Batch_norm_conv1') as scope:
    a_conv1, update_ema1 = batchnorm(a_conv1,tst,iter,b_conv1,convolutional=True)
    h_conv1 = tf.nn.relu(a_conv1)

with tf.name_scope("Conv2") as scope:
  W_conv2 = tf.get_variable("Conv_Layer_2", shape=[4, 1, num_filt_1, num_filt_2],initializer=initializer)  #8
  b_conv2 = bias_variable([num_filt_2], 'bias_for_Conv_Layer_2')
  a_conv2 = conv2d(h_conv1, W_conv2) + b_conv2

with tf.name_scope('Batch_norm_conv2') as scope:
    a_conv2, update_ema2 = batchnorm(a_conv2,tst,iter,b_conv2,convolutional=True)
    h_conv2 = tf.nn.relu(a_conv2)

with tf.name_scope("Fully_Connected1") as scope:
  W_fc1 = tf.get_variable("Fully_Connected_layer_1", shape=[N_FEATURES*num_filt_2, num_fc_1],initializer=initializer)
  b_fc1 = bias_variable([num_fc_1], 'bias_for_Fully_Connected_Layer_1')
  h_conv3_flat = tf.reshape(h_conv2, [-1, N_FEATURES*num_filt_2])
  h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)

with tf.name_scope("Fully_Connected2") as scope:
  h_fc1_drop = tf.nn.dropout(h_fc1, pkeep)
  W_fc2 = tf.get_variable("W_fc2", shape=[num_fc_1, num_classes],initializer=initializer)
  b_fc2 = tf.Variable(tf.constant(0.1, shape=[num_classes]),name = 'b_fc2')
  h_fc2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
  y = tf.nn.softmax(h_fc2)

with tf.name_scope("SoftMax") as scope:
#    regularizers = (tf.nn.l2_loss(W_conv1) + tf.nn.l2_loss(b_conv1) +
#                  tf.nn.l2_loss(W_conv2) + tf.nn.l2_loss(b_conv2) +
#                  tf.nn.l2_loss(W_conv3) + tf.nn.l2_loss(b_conv3) +
#                  tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(b_fc1) +
#                  tf.nn.l2_loss(W_fc2) + tf.nn.l2_loss(b_fc2))
    loss = tf.nn.softmax_cross_entropy_with_logits(logits=h_fc2,labels=y_)
    cost = tf.reduce_sum(loss) / BATCH_SIZE
#    cost += regularization*regularizers
    loss_summ = tf.summary.scalar("cross_entropy_loss", cost)

update_ema = tf.group(update_ema1, update_ema2)


with tf.name_scope("train") as scope:
    tvars = tf.trainable_variables()
    #We clip the gradients to prevent explosion
    grads = tf.gradients(cost, tvars)
    optimizer = tf.train.AdamOptimizer(learning_rate)
    gradients = list(zip(grads, tvars))
    train_step = optimizer.apply_gradients(gradients)


with tf.name_scope("Evaluating_accuracy") as scope:
    correct_prediction = tf.equal(tf.argmax(h_fc2,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    accuracy_summary = tf.summary.scalar("accuracy", accuracy)


merged = tf.summary.merge_all()

#def print_tvars():
#  tvars = tf.trainable_variables()
#  for variable in tvars:
#    print(variable.name)
#  return
#print_tvars()




data_batch, label_batch = batch_generator(["TRAININGRR20V4.csv"])


init = tf.global_variables_initializer() 


with tf.Session() as sess:
	sess.run(init)
	coord=tf.train.Coordinator()
	threads=tf.train.start_queue_runners(coord=coord)
	for i in range(20000):
    		features, labels = sess.run([data_batch, label_batch])
    		b=np.zeros((BATCH_SIZE,2))
    		b[np.arange(BATCH_SIZE), labels]=1
    		if i%200 == 0:
      			#Check training performance
      			train_accuracy = accuracy.eval(feed_dict = { X: features, y_: b, pkeep: 1.0, tst : False})
      			print("step %d, training accuracy %g"%(i, train_accuracy))
    		train_step.run(feed_dict={X: features, y_: b, pkeep: dropout, tst : True})
    		update_ema.run(feed_dict={X: features, y_: b, tst: False, iter: i, pkeep: 1.0})
	coord.request_stop()
	coord.join(threads)
	probabilities=y
	print ("probabilities", probabilities.eval(feed_dict={X: test_set.data, tst: False, pkeep: 1.0}))
	np.savetxt('resultRR20V4.txt', probabilities.eval(feed_dict={X: test_set.data, tst: False, pkeep: 1.0}), delimiter=',')
	
